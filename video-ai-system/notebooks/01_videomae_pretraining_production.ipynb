{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VideoMAE Pre-training (Production-Scale) on Something-Something-V2\n",
        "\n",
        "This notebook implements a full-scale, production-level self-supervised pre-training run using VideoMAE on the **Something-Something-V2 (SSv2)** dataset. This dataset is significantly larger and more complex than HMDB51, making it ideal for training a powerful, general-purpose video understanding model.\n",
        "\n",
        "**Key changes from the prototype:**\n",
        "- **Dataset:** Switched from HMDB51 to the large-scale Something-Something-V2 dataset.\n",
        "- **Training Duration:** Increased from 3 epochs to 800 epochs for comprehensive learning.\n",
        "- **Learning Rate Schedule:** Adjusted warmup period to 40 epochs to match the longer training cycle.\n",
        "- **Performance:** Enabled mixed-precision training (`fp16`) to accelerate the process and reduce GPU memory footprint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Environment\n",
        "\n",
        "First, we clone the MMAction2 repository and install all the required dependencies. We'll also set the GPU environment for Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Ensure we are using a GPU runtime\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Cloning MMAction2 repository...\")\n",
        "!git clone https://github.com/open-mmlab/mmaction2.git\n",
        "os.chdir('mmaction2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Installing dependencies...\")\n",
        "# Uninstall existing incompatible versions\n",
        "!pip uninstall mmcv -y\n",
        "!pip uninstall mmcv-full -y\n",
        "\n",
        "# Install the correct version of mmcv\n",
        "!pip install mmcv==2.1.0 -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0/index.html\n",
        "\n",
        "# Install other required packages\n",
        "!pip install -q decord einops timm\n",
        "\n",
        "# Install mmaction2 from source\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preparation\n",
        "\n",
        "Next, we download and extract the Something-Something-V2 dataset. This is a large dataset, so the download and extraction process will take a significant amount of time and disk space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a data directory\n",
        "!mkdir -p ../data/ssv2\n",
        "os.chdir('../data/ssv2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Downloading Something-Something-V2 videos (20GB). This will take a while...\")\n",
        "# Download the main video files (Note: The original file is a .zip, not .tgz)\n",
        "!wget https://s3.amazonaws.com/something-something-v2/20bn-something-something-v2-video.zip -O 20bn-something-something-v2-video.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Downloading train/validation splits and labels...\")\n",
        "# Download the official train/test splits and labels\n",
        "!wget https://s3.amazonaws.com/something-something-v2/20bn-something-something-v2-labels.json -O labels.json\n",
        "!wget https://s3.amazonaws.com/something-something-v2/20bn-something-something-v2-train.json -O train.json\n",
        "!wget https://s3.amazonaws.com/something-something-v2/20bn-something-something-v2-validation.json -O validation.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Extracting video files...\")\n",
        "# Extract the video files\n",
        "!unzip 20bn-something-something-v2-video.zip -d videos/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Data preparation complete.\")\n",
        "os.chdir('../../mmaction2') # Go back to mmaction2 directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Annotation Files\n",
        "\n",
        "We now parse the downloaded JSON files to create annotation lists in the format required by MMAction2 for pre-training. For self-supervised learning, we only need the video paths, not the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "print(\"Generating annotation files for MMAction2...\")\n",
        "\n",
        "data_root = '../data/ssv2/'\n",
        "video_root = os.path.join(data_root, 'videos')\n",
        "anno_dir = os.path.join(data_root, 'annotations')\n",
        "os.makedirs(anno_dir, exist_ok=True)\n",
        "\n",
        "def generate_ssv2_anno(json_path, output_path):\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    with open(output_path, 'w') as f_out:\n",
        "        for item in data:\n",
        "            video_id = item['id']\n",
        "            video_path = os.path.join('videos', f\"{video_id}.webm\")\n",
        "            # For pre-training, we don't need labels, but mmaction2 expects a placeholder (-1)\n",
        "            f_out.write(f\"{video_path} -1\\n\")\n",
        "\n",
        "# We use the training set for pre-training\n",
        "train_json_path = os.path.join(data_root, 'train.json')\n",
        "output_train_anno = os.path.join(anno_dir, 'ssv2_train_list_videos.txt')\n",
        "generate_ssv2_anno(train_json_path, output_train_anno)\n",
        "\n",
        "print(f\"Annotation file created at: {output_train_anno}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!echo \"Sample from annotation file:\"\n",
        "!head -n 5 {output_train_anno}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration\n",
        "\n",
        "We will now create a custom configuration file for the VideoMAE pre-training task on SSv2. This config inherits from the base VideoMAE config and overrides data paths and training parameters for our production-scale run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config_content = \"\"\"_base_ = [\\n    './configs/_base_/models/videomae_vit-base-p16.py',\\n    './configs/_base_/default_runtime.py'\\n]\\n\\n# model settings\\nmodel = dict(\\n    backbone=dict(drop_path_rate=0.1),\\n    neck=dict(type='VideoMAEPretrainNeck',\\n        embed_dims=768,\\n        patch_size=16,\\n        tube_size=2,\\n        decoder_embed_dims=384,\\n        decoder_depth=4,\\n        decoder_num_heads=6,\\n        mlp_ratio=4.,\\n        norm_pix_loss=True),\\n    head=dict(type='VideoMAEPretrainHead',\\n        norm_pix_loss=True,\\n        patch_size=16,\\n        tube_size=2))\\n\\n# dataset settings\\ndataset_type = 'VideoDataset'\\ndata_root = '../data/ssv2/'\\nann_file_train = '../data/ssv2/annotations/ssv2_train_list_videos.txt'\\n\\ntrain_pipeline = [\\n    dict(type='DecordInit'),\\n    dict(type='SampleFrames', clip_len=16, frame_interval=4, num_clips=1),\\n    dict(type='DecordDecode'),\\n    dict(type='Resize', scale=(-1, 256)),\\n    dict(type='RandomResizedCrop', area_range=(0.5, 1.0)),\\n    dict(type='Resize', scale=(224, 224), keep_ratio=False),\\n    dict(type='Flip', flip_ratio=0.5),\\n    dict(type='FormatShape', input_format='NCTHW'),\\n    dict(type='MaskingGenerator', mask_window_size=(8, 7, 7), mask_ratio=0.75),\\n    dict(type='Collect', keys=['imgs', 'mask'], meta_keys=()),\\n    dict(type='ToTensor', keys=['imgs', 'mask'])]\\n\\ndata = dict(\\n    videos_per_gpu=8, \\n    workers_per_gpu=4,\\n    train=dict(\\n        type=dataset_type,\\n        ann_file=ann_file_train,\\n        data_prefix=data_root,\\n        pipeline=train_pipeline))\\n\\n# optimizer\\noptimizer = dict(\\n    type='AdamW',\\n    lr=1.5e-4,\\n    betas=(0.9, 0.95),\\n    weight_decay=0.05)\\n\\n# learning policy\\nlr_config = dict(\\n    policy='CosineAnnealing',\\n    min_lr=0,\\n    warmup='linear',\\n    warmup_by_epoch=True,\\n    warmup_iters=40) # Increased warmup for longer run\\n\\ntotal_epochs = 800 # Production-scale training duration\\n\\n# runtime settings\\nwork_dir = './work_dirs/videomae_pretrain_ssv2_production'\\nlog_config = dict(interval=50)\\n\\n# enable mixed-precision training\\nfp16 = dict(loss_scale='dynamic')\\n\"\"\"\\n\\nconfig_path = './configs/recognition/videomae/videomae_pretrain_ssv2_production.py'\\nwith open(config_path, 'w') as f:\\n    f.write(config_content)\\n\\nprint(f\"Configuration file created at: {config_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run Pre-training\n",
        "\n",
        "With the data and configuration ready, we can now launch the pre-training script. This will be a long-running job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python tools/train.py \\\n",
        "    ./configs/recognition/videomae/videomae_pretrain_ssv2_production.py \\\n",
        "    --work-dir ./work_dirs/videomae_pretrain_ssv2_production \\\n",
        "    --validate \\\n",
        "    --seed 42 \\\n",
        "    --deterministic \\\n",
        "    --gpu-ids 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Results & Validation (Placeholder)\n",
        "\n",
        "After the full 800-epoch training run, this section would involve:\n",
        "\n",
        "1.  **Analyzing Loss Curves:** Plotting the training loss from the log files (e.g., using TensorBoard) to ensure the model was learning effectively over the long run.\n",
        "2.  **Visualizing Reconstructions:** Running inference on a few validation videos and visualizing the model's masked reconstructions to qualitatively assess its understanding of motion and appearance.\n",
        "3.  **Downstream Task Evaluation:** The ultimate validation is fine-tuning this pre-trained model on a downstream task (like action classification) and measuring its performance improvement compared to training from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ONNX Export\n",
        "\n",
        "Finally, we export the trained ViT encoder backbone to the ONNX format. This makes the model portable and ready for deployment in various inference environments. We will use the checkpoint from the final epoch of our training run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from mmaction.apis import init_recognizer\n",
        "from mmaction.core.deployment import torch2onnx\n",
        "\n",
        "print(\"Exporting model to ONNX...\")\n",
        "\n",
        "# Path to the config file we created\n",
        "config_file = './configs/recognition/videomae/videomae_pretrain_ssv2_production.py'\n",
        "\n",
        "# Path to the checkpoint file from the training run\n",
        "# NOTE: MMAction2 saves checkpoints as epoch_X.pth\n",
        "checkpoint_file = './work_dirs/videomae_pretrain_ssv2_production/epoch_800.pth'\n",
        "output_file = '../video_mae_encoder_ssv2_production.onnx'\n",
        "\n",
        "# Build the model from a config file and a checkpoint file\n",
        "model = init_recognizer(config_file, checkpoint_file, device='cpu')\n",
        "\n",
        "# We only want to export the encoder (backbone)\n",
        "encoder = model.backbone\n",
        "\n",
        "# Create a dummy input with the expected shape\n",
        "# (batch_size, num_channels, num_frames, height, width)\n",
        "dummy_input = torch.randn(1, 3, 16, 224, 224)\n",
        "\n",
        "# The torch2onnx function from MMAction2 handles the export\n",
        "torch.onnx.export(\n",
        "    encoder,\n",
        "    dummy_input,\n",
        "    output_file,\n",
        "    input_names=['input'],\n",
        "    output_names=['output'],\n",
        "    opset_version=11,\n",
        "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        ")\n",
        "\n",
        "print(f\"ONNX model saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ls -lh ../"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
