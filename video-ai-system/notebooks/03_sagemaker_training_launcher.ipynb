{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Production-Grade SageMaker Training & HPO Launcher\n",
        "\n",
        "This notebook provides a sophisticated and robust interface for launching and managing VideoMAE pre-training jobs and Hyperparameter Optimization (HPO) tasks on Amazon SageMaker. It is designed for production-level workflows, incorporating best practices for configuration, data handling, and job management."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key Features:**\n",
        "\n",
        "1.  **Centralized Configuration:** All parameters are defined in a single block for easy management.\n",
        "2.  **SageMaker Data Channels:** Uses SageMaker's native, efficient data channels instead of manual data syncing.\n",
        "3.  **Cost-Effective Spot Training:** Integrated support for SageMaker Managed Spot Instances.\n",
        "4.  **Live Log Streaming:** Training job logs are streamed directly into the notebook for real-time monitoring.\n",
        "5.  **Model Registry Integration:** A full workflow to register the best model from an HPO job into the SageMaker Model Registry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Global Configuration\n",
        "\n",
        "All user-configurable parameters are defined in this cell. Adjust these values to match your environment and training requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{\n",
        " \"cells\": [\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"# Production-Grade SageMaker Training & HPO Launcher\\n\",\n",
        "    \"\\n\",\n",
        "    \"This notebook provides a sophisticated and robust interface for launching and managing VideoMAE pre-training jobs and Hyperparameter Optimization (HPO) tasks on Amazon SageMaker. It is designed for production-level workflows, incorporating best practices for configuration, data handling, and job management.\\n\",\n",
        "    \"\\n\",\n",
        "    \"**Key Features:**\\n\",\n",
        "    \"1.  **Centralized Configuration:** All parameters are defined in a single block for easy management.\\n\",\n",
        "    \"2.  **SageMaker Data Channels:** Uses SageMaker's native, efficient data channels instead of manual data syncing.\\n\",\n",
        "    \"3.  **Cost-Effective Spot Training:** Integrated support for SageMaker Managed Spot Instances.\\n\",\n",
        "    \"4.  **Live Log Streaming:** Training job logs are streamed directly into the notebook for real-time monitoring.\\n\",\n",
        "    \"5.  **Model Registry Integration:** A full workflow to register the best model from an HPO job into the SageMaker Model Registry.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## 1. Global Configuration\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# All user-configurable parameters are defined in this cell. Adjust these values to match your environment and training requirements.\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Path to the tiny dataset for Free Tier validation\\n\",\n",
        "    \"S3_DATA_PATH = \\\"s3://miqa-data/ssv2-tiny\\\"\\n\",\n",
        "    \"\\n\",\n",
        "    \"S3_OUTPUT_PREFIX = 'videomae-free-tier-test'\\n\",\n",
        "    \"\\n\",\n",
        "    \"PROJECT_NAME = 'videomae-free-tier-test'\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Critical: Use a Free Tier eligible instance type\\n\",\n",
        "    \"INSTANCE_TYPE = 'ml.m5.xlarge'\\n\",\n",
        "    \"\\n\",\n",
        "    \"INSTANCE_COUNT = 1\\n\",\n",
        "    \"\\n\",\n",
        "    \"FRAMEWORK_VERSION = '2.0'\\n\",\n",
        "    \"\\n\",\n",
        "    \"PYTHON_VERSION = 'py310'\\n\",\n",
        "    \"\\n\",\n",
        "    \"SOURCE_DIR = '../scripts'\\n\",\n",
        "    \"\\n\",\n",
        "    \"ENTRY_POINT = 'train.py'\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Spot instances are not used for this test to ensure it runs\\n\",\n",
        "    \"USE_SPOT_INSTANCES = False\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Limit run time to 1 hour to stay within budget\\n\",\n",
        "    \"MAX_RUN_SECONDS = 3600\\n\",\n",
        "    \"\\n\",\n",
        "    \"MAX_WAIT_SECONDS = 3600 * 2 # Not used when spot is false, but set for safety\\n\",\n",
        "    \"\\n\",\n",
        "    \"# These keys MUST match the command-line arguments in the entry_point script (train.py).\\n\",\n",
        "    \"# For SageMaker, it's a best practice to use kebab-case for hyperparameters.\\n\",\n",
        "    \"HYPERPARAMETERS = {\\n\",\n",
        "    \"    'total-epochs': 1,\\n\",\n",
        "    \"    'learning-rate': 1.5e-4,\\n\",\n",
        "    \"    'batch-size': 2,\\n\",\n",
        "    \"    'warmup-epochs': 0\\n\",\n",
        "    \"}\\n\",\n",
        "    \"\\n\",\n",
        "    \"# HPO settings are no longer used\\n\",\n",
        "    \"# HPO_MAX_JOBS = 10\\n\",\n",
        "    \"# HPO_MAX_PARALLEL_JOBS = 2\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Model Registry settings are no longer used\\n\",\n",
        "    \"# MODEL_PACKAGE_GROUP_NAME = PROJECT_NAME\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## 2. Session Setup\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# This section initializes the SageMaker and Boto3 sessions and retrieves the necessary execution role. This notebook assumes it is being run from a SageMaker environment (like SageMaker Studio or a Notebook Instance) where the execution role is automatically configured.\\n\",\n",
        "    \"import sagemaker\\n\",\n",
        "    \"import boto3\\n\",\n",
        "    \"import os\\n\",\n",
        "    \"import time\\n\",\n",
        "    \"from sagemaker.pytorch import PyTorch\\n\",\n",
        "    \"# Tuner is no longer used\\n\",\n",
        "    \"# from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\\n\",\n",
        "    \"\\n\",\n",
        "    \"sagemaker_session = sagemaker.Session()\\n\",\n",
        "    \"role = sagemaker.get_execution_role()\\n\",\n",
        "    \"region = sagemaker_session.boto_region_name\\n\",\n",
        "    \"s3_output_path = f\\\"s3://{sagemaker_session.default_bucket()}/{S3_OUTPUT_PREFIX}\\\"\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(f\\\"SageMaker SDK Version: {sagemaker.__version__}\\\")\\n\",\n",
        "    \"print(f\\\"Region: {region}\\\")\\n\",\n",
        "    \"print(f\\\"IAM Role: {role}\\\")\\n\",\n",
        "    \"print(f\\\"S3 Data Input Path: {S3_DATA_PATH}\\\")\\n\",\n",
        "    \"print(f\\\"S3 Model Output Path: {s3_output_path}\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## 3. Define the SageMaker PyTorch Estimator\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# This estimator is the core component that defines our training environment. \\n\",\n",
        "    \"# It will be used for our single training job.\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Define metrics that SageMaker will parse from the training job logs.\\n\",\n",
        "    \"metric_definitions = [\\n\",\n",
        "    \"    {'Name': 'train:loss', 'Regex': 'Training-Loss: ([0-9\\\\\\\\.]+)'}\\n\",\n",
        "    \"]\\n\",\n",
        "    \"\\n\",\n",
        "    \"estimator = PyTorch(\\n\",\n",
        "    \"    entry_point=ENTRY_POINT,\\n\",\n",
        "    \"    source_dir=SOURCE_DIR,\\n\",\n",
        "    \"    role=role,\\n\",\n",
        "    \"    instance_count=INSTANCE_COUNT,\\n\",\n",
        "    \"    instance_type=INSTANCE_TYPE,\\n\",\n",
        "    \"    framework_version=FRAMEWORK_VERSION,\\n\",\n",
        "    \"    py_version=PYTHON_VERSION,\\n\",\n",
        "    \"    hyperparameters=HYPERPARAMETERS,\\n\",\n",
        "    \"    output_path=s3_output_path,\\n\",\n",
        "    \"    metric_definitions=metric_definitions,\\n\",\n",
        "    \"    use_spot_instances=USE_SPOT_INSTANCES,\\n\",\n",
        "    \"    max_run=MAX_RUN_SECONDS,\\n\",\n",
        "    \"    max_wait=MAX_WAIT_SECONDS if USE_SPOT_INSTANCES else None\\n\",\n",
        "    \")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## 4. Launch a Single Training Job (Free Tier Test)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# The HPO and Model Registration steps have been removed for this cost-effective test.\\n\",\n",
        "    \"# We will now launch a single, standard training job using the estimator defined above.\\n\",\n",
        "    \"import time\\n\",\n",
        "    \"\\n\",\n",
        "    \"training_job_name = f\\\"{PROJECT_NAME}-test-{int(time.time())}\\\"\\n\",\n",
        "    \"print(f\\\"Launching a single training job on a Free Tier instance: {training_job_name}\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"try:\\n\",\n",
        "    \"    # Use the 'estimator' you configured in the cell above\\n\",\n",
        "    \"    estimator.fit({'training': S3_DATA_PATH}, job_name=training_job_name, wait=True)\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    print(\\\"\\\\nJob completed successfully!\\\")\\n\",\n",
        "    \"    print(f\\\"Model artifacts saved to: {estimator.model_data}\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"except Exception as e:\\n\",\n",
        "    \"    print(f\\\"\\\\nError launching training job: {e}\\\")\"\n",
        "   ]\n",
        "  }\n",
        " ],\n",
        " \"metadata\": {\n",
        "  \"kernelspec\": {\n",
        "   \"display_name\": \"Python 3 (PyTorch 2.0 Python 3.10 CPU Optimized)\",\n",
        "   \"language\": \"python\",\n",
        "   \"name\": \"pytorch-2.0-cpu-py310\"\n",
        "  },\n",
        "  \"language_info\": {\n",
        "   \"codemirror_mode\": {\n",
        "    \"name\": \"ipython\",\n",
        "    \"version\": 3\n",
        "   },\n",
        "   \"file_extension\": \".py\",\n",
        "   \"mimetype\": \"text/x-python\",\n",
        "   \"name\": \"python\",\n",
        "   \"nbconvert_exporter\": \"python\",\n",
        "   \"pygments_lexer\": \"ipython3\",\n",
        "   \"version\": \"3.10.12\"\n",
        "  }\n",
        " },\n",
        " \"nbformat\": 4,\n",
        " \"nbformat_minor\": 4\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Session Setup\n",
        "\n",
        "This section initializes the SageMaker and Boto3 sessions and retrieves the necessary execution role. This notebook assumes it is being run from a SageMaker environment (like SageMaker Studio or a Notebook Instance) where the execution role is automatically configured."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sagemaker\n",
        "import boto3\n",
        "import os\n",
        "import time\n",
        "from sagemaker.pytorch import PyTorch\n",
        "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\n",
        "\n",
        "sagemaker_session = sagemaker.Session()\n",
        "role = sagemaker.get_execution_role()\n",
        "region = sagemaker_session.boto_region_name\n",
        "s3_output_path = f\"s3://{sagemaker_session.default_bucket()}/{S3_OUTPUT_PREFIX}\"\n",
        "\n",
        "print(f\"SageMaker SDK Version: {sagemaker.__version__}\")\n",
        "print(f\"Region: {region}\")\n",
        "print(f\"IAM Role: {role}\")\n",
        "print(f\"S3 Data Input Path: {S3_DATA_PATH}\")\n",
        "print(f\"S3 Model Output Path: {s3_output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define the SageMaker PyTorch Estimator\n",
        "\n",
        "This estimator is the core component that defines our training environment. It will be used for both single training jobs and as the base for our hyperparameter tuning job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define metrics that SageMaker will parse from the training job logs.\n",
        "metric_definitions = [\n",
        "    {'Name': 'train:loss', 'Regex': 'Training-Loss: ([0-9\\.]+)'}\n",
        "]\n",
        "\n",
        "estimator = PyTorch(\n",
        "    entry_point=ENTRY_POINT,\n",
        "    source_dir=SOURCE_DIR,\n",
        "    role=role,\n",
        "    instance_count=INSTANCE_COUNT,\n",
        "    instance_type=INSTANCE_TYPE,\n",
        "    framework_version=FRAMEWORK_VERSION,\n",
        "    py_version=PYTHON_VERSION,\n",
        "    hyperparameters=HYPERPARAMETERS,\n",
        "    output_path=s3_output_path,\n",
        "    metric_definitions=metric_definitions,\n",
        "    use_spot_instances=USE_SPOT_INSTANCES,\n",
        "    max_run=MAX_RUN_SECONDS,\n",
        "    max_wait=MAX_WAIT_SECONDS if USE_SPOT_INSTANCES else None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Launch a Hyperparameter Tuning Job\n",
        "\n",
        "To find the best hyperparameters automatically, we use SageMaker's `HyperparameterTuner`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The keys in this dictionary must match the hyperparameter names defined in the estimator.\n",
        "hyperparameter_ranges = {\n",
        "    'learning-rate': ContinuousParameter(1e-5, 1e-3),\n",
        "    'batch-size': IntegerParameter(4, 16)\n",
        "}\n",
        "\n",
        "tuner = HyperparameterTuner(\n",
        "    estimator=estimator,\n",
        "    objective_metric_name='train:loss',\n",
        "    hyperparameter_ranges=hyperparameter_ranges,\n",
        "    objective_type='Minimize',\n",
        "    max_jobs=HPO_MAX_JOBS,\n",
        "    max_parallel_jobs=HPO_MAX_PARALLEL_JOBS,\n",
        "    base_tuning_job_name=f\"{PROJECT_NAME}-hpo\"\n",
        ")\n",
        "\n",
        "tuning_job_name = f\"{PROJECT_NAME}-hpo-{int(time.time())}\"\n",
        "print(f\"Launching hyperparameter tuning job: {tuning_job_name}\")\n",
        "print(f\"Check progress: https://{region}.console.aws.amazon.com/sagemaker/home?region={region}#/hyper-tuning-jobs/{tuning_job_name}\")\n",
        "\n",
        "try:\n",
        "    tuner.fit({'training': S3_DATA_PATH}, job_name=tuning_job_name, wait=True)\n",
        "except Exception as e:\n",
        "    print(f\"\\nError launching tuning job: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Analyze Tuning Results and Register the Best Model\n",
        "\n",
        "Once the tuning job is complete, we find the best-performing training job and register its model artifacts into the SageMaker Model Registry for versioning and deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    tuner_analyzer = sagemaker.analytics.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
        "    best_job_name = tuner_analyzer.best_training_job()['TrainingJobName']\n",
        "    print(f\"Best training job found: {best_job_name}\")\n",
        "\n",
        "    best_estimator = PyTorch.attach(best_job_name)\n",
        "    model_artifacts = best_estimator.model_data\n",
        "    print(f\"Model artifacts for the best job are at: {model_artifacts}\")\n",
        "\n",
        "    sm_client = sagemaker_session.sagemaker_client\n",
        "    try:\n",
        "        sm_client.create_model_package_group(\n",
        "            ModelPackageGroupName=MODEL_PACKAGE_GROUP_NAME,\n",
        "            ModelPackageGroupDescription=f\"Models for {PROJECT_NAME}\"\n",
        "        )\n",
        "        print(f\"Created Model Package Group: {MODEL_PACKAGE_GROUP_NAME}\")\n",
        "    except sm_client.exceptions.ClientError as e:\n",
        "        if 'Name already exists' in str(e):\n",
        "            print(f\"Model Package Group '{MODEL_PACKAGE_GROUP_NAME}' already exists.\")\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "    model_package = best_estimator.register(\n",
        "        content_types=[\"application/x-video\"],\n",
        "        response_types=[\"application/json\"],\n",
        "        inference_instances=[\"ml.g4dn.xlarge\"],\n",
        "        transform_instances=[\"ml.m5.large\"],\n",
        "        model_package_group_name=MODEL_PACKAGE_GROUP_NAME,\n",
        "        approval_status=\"PendingManualApproval\"\n",
        "    )\n",
        "    print(f\"\\nSuccessfully registered model version: {model_package.model_package_arn}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Could not analyze or register model. It may have failed. Error: {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (PyTorch 2.0 Python 3.10 CPU Optimized)",
      "language": "python",
      "name": "pytorch-2.0-cpu-py310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
